---
title: 'Homework #2'
author: "Katie Daisey"
date: "Thursday, March 05, 2015"
output: html_document
---

1-comment on graph

2- a)add legend
2-d - tests for randomness

3 - measure variances for scaling
5-remove outliers before data distance


```{r echo=FALSE}
library(knitr)
library(lattice)
opts_chunk$set(fig.path="figure/",fig.width=10,fig.height=8,fig.align="center",
               message=FALSE,error=FALSE,warning=FALSE)
screen_standard<-par(mar=c(5,4,4,2)+0.1,fig=c(0,1,0,1),mfrow=c(1,1))

```


##Question 1 - Comparing Methods

New method development requires a comparision to the old or accepted method.  In the case of glucose, a newly developed method involving direct measurement by an implanted electrochemical sensor needs to be compared to the standard method (a kinetic analysis using glucose oxidase and spectroscopy).  

The data, containing 111 observations of glucose levels using both methods, was downloaded and read into R.

```{r}
glucose<-read.csv("data/glucose.csv")
```


The standard method two compare two techniques is the ["Bland-Altman" plot] [1](1).  By plotting the difference between the techniques versus the means and checking for a strong regression with a slope of 0, Bland and Altman argue that a measurement of correlation between the two techniques without bias can be made.

Here, code from [Ken Kleinman][2] was adapted to create a function `BAplot()` which produces a Bland-Altman plot.(2)
```{r}
BAplot <- function(x,y,xmeth=deparse(substitute(x)),ymeth=deparse(substitute(y))){
  #adapted from Ken Kleinman 
  #http://www.r-bloggers.com/example-9-34-bland-altman-type-plot/
    #removed standardization and diff=0 line
    #added regression line and title options

  bamean = (x+y)/2
  badiff = (y-x)
  reg_line<-summary(lm(badiff~bamean))
  reg_slope<-round(reg_line[[4]][2],4)
  reg_incpt<-round(reg_line[[4]][1],4)
  
  
  plot(badiff~bamean, pch=20, xlab="mean", ylab="difference")
# in the following, the deparse(substitute(varname)) retrieves name of argument
  title(main=paste("Bland-Altman plot of x and y\n",
    xmeth, "and", ymeth
    ), adj=".5")
      text(280,-70,col="red",paste("y = ",reg_slope,"x + ",reg_incpt,sep=""))
#construct the reference lines on the fly
  abline(h = c( mean(badiff)+1.96 * sd(badiff),
    mean(badiff)-1.96 * sd(badiff), mean(badiff)), lty=2)

    abline(lm(badiff~bamean),col="red")
} 
```

Applying this function to the glucose data comparing the sensor measurements to the accepted veneous glucose measurements produces the following plot:
```{r}
BAplot(glucose$Sensor,glucose$Venous_glucose,"Sensor","Venous Glucose")
```



##Question 2 - Modeling Data


Measurements of the concentration absorbance of trace cynaide in waste water samples was gathered by [Meier and Zeund][3](3). 

####a) Linear Least-Squares Model

The data needed some basic pre-processing to be read into R.  It was copied and pasted into a text file and the first line (column names) was removed.

```{r}
#read data
    conc<-read.table("data/conc.txt",header=T,sep=" ")
    colnames(conc)<-c("Concentration","Absorbance")
```


We wish to fit a linear model of the form $y = mx + b$ to the data. This can be done with the `lm()` function in base-R.  Here we specify that there is a linear relationship between absorbance and concentration.

```{r}
#use base least squares regression
fit.linear<-lm(Absorbance~Concentration,conc)
intercept<-fit.linear[[1]][[1]]
slope<-fit.linear[[1]][[2]]

fit.linear
```
We see that the found model has a slope of `r slope` and an intercept of `r intercept`.  We can overlay this model on data and examine the goodness of fit.

```{r}
plot(conc$Concentration,conc$Absorbance,
     xlab=expression(paste("Concentration (",mu,"g/100mL)")),ylab="Absorbance",
     main="Trace Cynaide in Waste Water")
abline(lm(Absorbance~Concentration,conc),col="red",lty=2)

```



####b) Linear Least-Squares Residuals

It appears that our linear least squares model fits the data well, but we can quantify this belief.

```{r}
(s.fit.linear<-summary(fit.linear))
#r squared value - standard
```


The usual parameter is the $R^2$ value, here calculated as `r round(s.fit.linear[[8]],4)`.  Our line has a slope of `r round(s.fit.linear[[4]][[2]],5)` and an intercept of `r round(s.fit.linear[[4]][[1]],5)`.

We can also look at the residuals directly. A least squares equation is found by minimizing the residuals (ie finding the line that is the closest to all the points) so the residuals should be small and randomly spread above and below the line.


```{r}
#find and plot residuals from the linear fit
fit.linear.res<-resid(fit.linear)
par(mar=c(5,4,4,2)+0.1,fig=c(0,1,0,1),mfrow=c(1,1))
plot(conc[,1], fit.linear.res,  ylim=c(-0.020,0.020),
     xlab=expression(paste("Concentration (",mu,"g/100mL)")),ylab="Residuals", main="Residuals of Least Squares Fit\n of Absorbance on Concentration") 
abline(0,0,col="red",lty=2)
```

Taking a look at this plot, the residuals are small, but the residuals tend to be positive in the middle concentrations and negative at high concentrations. This means our model under estimates absorbance in the middle concentrations and over estimates absorbance at high concentrations.  There is signifigant lack of fit.


####c) Additional Terms

This curvature suggests we should investigate a quadratic model by adding a concentration-squared term, `Conc_sq`.

```{r}
#adding square term
    conc[,3]<-(conc[,1]^2)
    colnames(conc)<-c("Concentration","Absorbance","Conc_sq")

fit.quad<-lm(Absorbance~(Concentration+Conc_sq),data=conc)
intercept<-fit.quad[[1]][[1]]
slope<-fit.quad[[1]][[2]]
sq_term<-fit.quad[[1]][[3]]
(s.fit.quad<-summary(fit.quad))
```




```{r}
#calculate predicted values
#R will not use multiple terms when plotting abline(lm(),...)
pred.quad<-as.data.frame(matrix(seq(-10,250,length.out=10000),ncol=1))
pred.quad[,2]<-(intercept+pred.quad[,1]*slope + pred.quad[,1]*pred.quad[,1]*sq_term)

# plot
par(mar=c(5,4,4,2)+0.1,fig=c(0,1,0,1),mfrow=c(1,1))
plot(conc$Concentration,conc$Absorbance,
         xlab=expression(paste("Concentration (",mu,"g/100mL)")),ylab="Absorbance",
         main="Trace Cynaide in Waste Water")
    abline(lm(Absorbance~Concentration,conc),col="red",lty=2)
    points(pred.quad[,1],pred.quad[,2],type="l",col="blue",lty=1)
    legend("bottomright",bty="n",pch=c(1,NA,NA), lty=c(NA,2,1),
          col=c("black","red","blue"), legend = c("Data","Linear","Quadratic"),cex=1)

# calculate position of inset
    plotdim <- par("plt")
    xleft    = plotdim[1] + 0.02
    xright   = plotdim[1] + (plotdim[2] - plotdim[1])*.30 + 0.02
    ybottom  = plotdim[4] - (plotdim[4] - plotdim[3])*.40 - 0.03
    ytop     = plotdim[4] - 0.03  #

# set position for inset
par(
  fig = c(xleft, xright, ybottom, ytop)
  , mar=c(0,0,0,0)
  , new=TRUE
  )

# add inset
plot(conc$Concentration,conc$Absorbance,xlim=c(0,11),ylim=c(0,.06),xaxt='n',yaxt='n')
    abline(lm(Absorbance~Concentration,conc),col="red",lty=2)
    points(pred.quad[,1],pred.quad[,2],type="l",col="blue",lty=1)
```

The new $R^2$ value is  with our line having a coefficients of `r round(slope,5)` `r round(sq_term,5)`and  and an intercept of `r round(intercept,5)`.  

While it's not immediately obvious that the lines are distinct, more so in the inset, it becomes clearer if we examine the residuals.

```{r}
fit.quad.res<-resid(fit.quad)
par(mar=c(5,4,4,2)+0.1,fig=c(0,1,0,1),mfrow=c(1,1))
plot(conc[,1], fit.quad.res,  ylim=c(-0.015,0.015),
     xlab=expression(paste("Concentration (",mu,"g/100mL)")),ylab="Residuals", main="Residuals of Least Squares Fit\n of Absorbance on Concentration") 
abline(0,0,col="blue",lty=2)
```
Our $R^2$ has increased, but more importantly a visual examination of our residuals reveals no obvious pattern.

####d) Testing Residuals

Formally, we might like to describe the "randomness" of our residuals.


##Question 3 - Exploring Data
We might wish to explore data before testing an hypotheses.  We wish to perform an exploratory data analysis (EDA) on some various properties of elements to see if we can extract periodic trends.


```{r}
elem<-read.table("data/homework2-1.csv",sep=",",row.names=1,header=T)
```

####a)Scatterplot
```{r}
plot(elem)
```

####b)Boxplots
```{r}
boxplot(elem)
```

####c)Hierarchical Cluster Analysis
When looking for trends, we want to determine which elements are similar when we take into account all of the variables or which are closest when we plot them in a high-dimensional space.

To do so we use hierarchical cluster analysis, which groups closest neighbors together and keeps track at what distance the groups merged, with varying methods of determining distance between groups.  We also want to account for our variables having different units and therefore differing scales for means and spreads which will bias the distance measurements towards the larger valued variables.  We can center (mean = 0) and scale (sd = 1) the data to eliminate this bias (if desirable).

```{r}
elem_c<-scale(elem, scale=F) #centered only
elem_a<-scale(elem, scale=T) #centered and autoscaled
#hcluster on c data complete ward.D
hc_c_complete<-hclust(dist(elem_c),method="complete")
hc_c_ward<-hclust(dist(elem_c),method="ward.D")
#on autoscale data complete ward.D
hc_a_complete<-hclust(dist(elem_a),method="complete")
hc_a_ward<-hclust(dist(elem_a),method="ward.D")

```

Here we have performed "complete" (measuring distance between the two fartest points in each group) and "ward" (measuring distance between centroids with a scaling factor) hierarchical cluster analyses on centered and autoscaled (centered and scaled) data.

```{r}
par(mfrow=c(2,2))
plot(hc_c_complete, main="Centered - Complete", xlab=NA)
abline(h=3750,lty=2,col=2)
plot(hc_c_ward, main="Centered - Ward",xlab=NA)
abline(h=7000,lty=2, col=2)
plot(hc_a_complete, main="Autoscaled - Complete", xlab=NA)
abline(h=3,lty=2, col=2)
plot(hc_a_ward, main="Autoscaled - Ward", xlab=NA)
abline(h=5,lty=2, col=2)
```

We can see the centered data has a much larger separation between the largest two groups than the autoscaled data but this is a sign that the range of at least one of the variables is much larger than the others, not that these provide a "better" separation of the elements.

To find groups we look for a large difference in height between when two groups merge and when those groups were themselves created.  For example, in the centered-complete analysis, we see there are clearly two separate groups (Bi, Pb, Tl, Zn, Cu, Co, Ni, Fe) and (Br, I, F, Ar, He, Ne, Cl, Kr, Xe, Li, Na, K, Be, Rb, Cs, Sr, Mg, Ca).  We could argue a separation of 5 groups with a height of approximately 3750, but the choice of height for separation is arbitrary (as it is for all heirarchical cluster analyses).

The centered-ward analysis very clearly separates the same two groups from the centered-complete analysis, but even less clearly separates further groups.  We could argue for the same 5 groups.

The autoscaled analyses seem to provide more clearly separated groups by accounting for the larged differences in the ranges of the variables.  The autoscaled-complete analysis finds 5 distinct groups cleanly.  The autoscaled-ward analysis finds 6 very clear groups.  I would argue that the autoscaled analyses work better here because the we have no a priori reason to bias the data towards the variable with the large range.


Looking at the elements contained in each group 


####d)Principal Components Analysis

Using a principal components analysis (PCA), we can separate transform the data so the axes lie upon the line of maximum variance.  This can help us to identify and visalize separation in the data.  Using autoscaled elemental data we perform a PCA and plot the eigenvalues to determine how many principal components represent unique information in our data and how many represent noise.
```{r fig.width=5,fig.height=5.25}
elem_a_pc<-princomp(elem_a,cor=TRUE)
screeplot(elem_a_pc,type="lines", col="red", main ="PCA of Element Data")
```

```{r}
variances <-elem_a_pc$sdev^2/(nrow(elem_a)-1)
totvariances<- sum(variances)
relvars<- variances/totvariances
vars<- 100*round(relvars, digits=3)

plot(elem_a_pc$scores[,1],elem_a_pc$scores[,2],
     xlab=paste("PC#1(",vars[1],"%)", sep=" "),
     ylab=paste("PC#2(",vars[2],"%)",sep=" "),
     main="Scores of Element Data",pch=elem$Group, col=elem$Group)
abline(h=0, col="gray");abline(v=0, col="gray")
text(elem_a_pc$scores[,1], elem_a_pc$scores[,2],labels=rownames(elem),cex=.5, pos=1)
```

```{r}
plot(elem_a_pc$loadings[,1],elem_a_pc$loadings[,2],
     main="Loadings of Element Data",pch=elem$Group, col=elem$Group)
abline(h=0, col="gray");abline(v=0, col="gray")
text(elem_a_pc$loadings[,1], elem_a_pc$loadings[,2],labels=colnames(elem),cex=.5, pos=1)
```


##Question 4 - PCA for Identification

A common use for PCA is identification of samples via comparision to a known dataset.  Here we are examining a series of two-types optical filters, made and measured at NIST, where some have gone unlabeled ('Y'-type).

####a) Load and Examine data
```{r}
opt<-read.csv("data/homework2-3.csv")
plot(opt)
```
A quick look at the data reveals a strong correlation and separation in the P-variables.  Type would be excluded from the PCA. 

```{r}
str(opt)
summary(opt)
(apply(opt[,3:9],2,var))
```
The data needs to be centered, but looking at the variance, I would argue it should not be scaled.

####b) Exclude "Y" and Pre-process data
```{r}
#split out "unknown/type-y data"
opt2<-opt[opt$Type != "Y",]
opty<-opt[opt$Type == "Y",]

opt3<-opt2[,3:9]
opt3_means<-colMeans(opt3)
opt3_var<-apply(opt3,2,var)
opt3<-scale(opt3, scale=F)
```

####c) Preform PCA. Plot scores and labels
```{r}
opt3_c_pc<-princomp(opt3,cor=T)
plot(opt3_c_pc)

variances <-opt3_c_pc$sdev^2/(nrow(opt3)-1)
totvariances<- sum(variances)
relvars<- variances/totvariances
vars<- 100*round(relvars, digits=3)


plot(opt3_c_pc$scores[,1],opt3_c_pc$scores[,2], col=as.factor(opt2$Type), xlab=paste("PC#1 - ",vars[1],"% of Variance"), ylab=paste("PC#2 - ",vars[2],"% of Variance"))
abline(h=0, col="gray")
abline(v=0, col="gray")
text(opt3_c_pc$scores[,1],opt3_c_pc$scores[,2],labels=rownames(opt3),cex=.5, pos=3)
legend("bottom",inset=0,legend=c("2305","2305a"),col=c("black","red"), pch=1,cex=1)
```

####d) Loadings
```{r}
plot(opt3_c_pc$loadings[,1],opt3_c_pc$loadings[,2])
abline(h=0, col="gray")
abline(v=0, col="gray")
text(opt3_c_pc$loadings[,1],opt3_c_pc$loadings[,2],labels=colnames(opt3),cex=.5,pos=c(4,4,4,4,4,4,2))
```
Loadings which are different from 0, contribute to the differences in the filters.  If only one measurement could be taken, P3 would be optimal.  P7 contributes the least to the difference in filters along the first principal component, which explains an overwhelming amount (96%) of the variance.

####e) Project Y samples onto PCA model

We need to pre-process the "Y"-type filters identically to the known filters (using the same mean and variances for scaling as necessary).
```{r project}
opty<-opt[opt$Type=="Y",]
scalec<-attributes(opt3)$"scaled:center"
opty_c<-opty[,3:9]
opty_c<-scale(opty_c,center=scalec,scale=F)
```

We then project the data onto the new principal component axes found with the known data using the loadings.

```{r}
newscores<-opty_c%*%opt3_c_pc$loadings
```

Finally we plot the new data as see which type it more closely resembles.
```{r}
plot(opt3_c_pc$scores[,1],opt3_c_pc$scores[,2], col=as.factor(opt2$Type), xlab=paste("PC#1 - ",vars[1],"% of Variance"), ylab=paste("PC#2 - ",vars[2],"% of Variance"))
abline(h=0, col="gray")
abline(v=0, col="gray")
text(opt3_c_pc$scores[,1],opt3_c_pc$scores[,2],labels=rownames(opt3),cex=.5, pos=3)
legend("bottom",inset=0,legend=c("2305","2305a","Y"),col=c("black","red","green"), pch=1,cex=1)
points(newscores[,1],newscores[,2],col=3)
```
Here, the "Y"-type filters closely rersemble "2305"-type filters, but do seem to have slight differences, perhaps as a separate production run, however, they are clearly not "2305a"-type filters.


##Question 5 - Determining differences between Fire Ants

####a) Load and Preprocess Data
```{r}
ants<-read.csv("data/homework2-4.csv")
summary(ants)
```
A quick look at the data shows the type of ant (Forager/Reserver) is located in column 1 and should be removed for PCA.

```{r}
ants2<-as.matrix(ants[,2:6])
apply(ants2,2,var)
```
The variances are wide enough that autoscale may be appropriate.

```{r}
ants2_a<-scale(ants2)
```

####b) Distance between ants and centroid

####c) Determine Malhalanobis Distances

####d) Calculate Assignments using Distance

####e) PCA
```{r}


ants<-read.csv("data/homework2-4.csv")
ants_forager<-ants[ants$Type=="Forager",2:6]
ants_reserver<-ants[ants$Type=="Reserver",2:6]
ants_forg_cent<-colMeans(ants_forager)
ants_res_cent<-colMeans(ants_reserver)

#it looks likely that there is an outlier
plot(ants_forager[,1]~ants_forager[,2])
points(ants_forg_cent[1],ants_forg_cent[2],col="red")


plot(ants_reserver[,1]~ants_reserver[,2])
points(ants_res_cent[1],ants_res_cent[2],col="red")



#outlier identified and removed
plot(ants[,2],ants[,3])
text(ants[,2],ants[,3],labels=rownames(ants))
ants_out<-ants[-1,]
ants_forager<-ants_out[ants_out$Type=="Forager",2:6]
ants_reserver<-ants_out[ants_out$Type=="Reserver",2:6]
ants_forg_cent<-colMeans(ants_forager)
ants_res_cent<-colMeans(ants_reserver)

#now it's gone!
plot(ants_forager[,1]~ants_forager[,3])
points(ants_forg_cent[1],ants_forg_cent[3],col="red")


plot(ants_reserver[,1]~ants_reserver[,2])
points(ants_res_cent[1],ants_res_cent[2],col="red")



e_dist<-function(point,y){
    dist_vect<-NULL
    for(i in 1:length(y[,1])){
            a<-y[i,]-point
            a<-a*a
            a<-sqrt(sum(a))
            dist_vect<-append(dist_vect,a)
        }
    dist_vect
    }


edist_for<-e_dist(ants_forg_cent, ants[,2:6])
edist_res<-e_dist(ants_res_cent, ants[,2:6])

plot(edist_for,edist_res,col=ants_out$Type)

#dist calculates a distance matrix
#if we want only the distances from a certain point
#we can put that point in the first row to makes a (n+1 x p) matrix
#and select only the first n distances calcualted
ants2<-ants_out[,2:6]
ants2<-rbind(ants_forg_cent,ants2)
a<-length(ants_out[,1])
b<-head(dist(ants2),a)

ants2<-ants_out[,2:6]
ants2<-rbind(ants_res_cent,ants2)
a<-length(ants_out[,1])
b<-head(dist(ants2),a)



#mahalanobis distance
#sqrt(x-u)Sx(x-u)
ants2<-ants[,2:6]
ants_cov<-cov(ants2)
mdist_for<-mahalanobis(ants2,ants_forg_cent,ants_cov)
mdist_res<-mahalanobis(ants2,ants_res_cent,ants_cov)

plot(b,d,col=ants$Type)




#for each distance type
#chose smaller distance and assign that type
#find percentage of true

euc<-as.data.frame(cbind(ants$Type,edist_for,edist_res,2,0))
colnames(euc)<-c("Type","edist_for","edist_res","Predict","Match")

euc[edist_for<edist_res,4]<-1
euc[euc$Type==euc$Predict,5]<-1
summary(euc[,5])[4]

plot(edist_for,edist_res,col=euc$Type,pch=euc$Predict,xlim=c(0,10),ylim=c(0,11))


mah<-as.data.frame(cbind(ants$Type,mdist_for,mdist_res,2,0))
colnames(mah)<-c("Type","mdist_for","mdist_res","Predict","Match")

mah[mdist_for<mdist_res,4]<-1
mah[mah$Type==mah$Predict,5]<-1
summary(mah[,5])[4]

plot(mdist_for,mdist_res,col=mah$Type,pch=mah$Predict,xlim=c(0,12),ylim=c(0,14))

```

##References
[1]: http://www.sciencedirect.com/science/article/pii/S0140673695917489 "JM Bland and DG Altman, Lancet (1995) 346(8982) 1085-1087" 
1. "JM Bland and DG Altman, Lancet (1995) 346(8982) 1085-1087" 

[2]: http://www.r-bloggers.com/example-9-34-bland-altman-type-plot/ 
"Kleinman, K. "Example 9.34: Bland-Altman type plot" June 5, 2012." 
2. Kleinman, K. "Example 9.34: Bland-Altman type plot" June 5, 2012.  http://www.r-bloggers.com/example-9-34-bland-altman-type-plot/

[3]:
3. 